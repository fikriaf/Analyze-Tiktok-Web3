{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Analisis Sentimen TikTok: Persepsi Publik terhadap Isu Sosial Web3\n",
        "\n",
        "Implementasi lengkap metodologi penelitian sesuai proposal.\n",
        "\n",
        "**Tahapan:**\n",
        "1. Setup dan Import Libraries\n",
        "2. Pengumpulan Data (TikTok Scraping)\n",
        "3. Pra-Pemrosesan Data\n",
        "4. Analisis Sentimen (Rule-based)\n",
        "5. Analisis Trending Topic\n",
        "6. Visualisasi Wordcloud\n",
        "7. Analisis Kritis dan Kesimpulan\n",
        "\n",
        "**Semua output akan disimpan ke folder `output/`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Setup selesai\n",
            "✓ Folder output dibuat\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "# !pip install pandas numpy matplotlib seaborn wordcloud TikTokApi playwright requests\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Setup matplotlib\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette('husl')\n",
        "%matplotlib inline\n",
        "\n",
        "# Create output directories\n",
        "os.makedirs('output', exist_ok=True)\n",
        "os.makedirs('output/wordclouds', exist_ok=True)\n",
        "os.makedirs('output/graphs', exist_ok=True)\n",
        "os.makedirs('output/data', exist_ok=True)\n",
        "\n",
        "print('✓ Setup selesai')\n",
        "print('✓ Folder output dibuat')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TAHAP 1: Pengumpulan Data dari TikTok\n",
        "\n",
        "**Target Hashtags:**\n",
        "- #AIethics\n",
        "- #blockchain\n",
        "- #sustainability\n",
        "- #web3\n",
        "- #digitalfreedom\n",
        "- #cryptocurrency\n",
        "- #NFT\n",
        "- #metaverse\n",
        "- #privacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fungsi scraping TikTok\n",
        "def scrape_tiktok_data(hashtags, videos_per_hashtag=100):\n",
        "    \"\"\"\n",
        "    Scrape data dari TikTok berdasarkan hashtag\n",
        "    \n",
        "    Parameters:\n",
        "    - hashtags: list of strings\n",
        "    - videos_per_hashtag: int, jumlah video per hashtag\n",
        "    \n",
        "    Returns:\n",
        "    - DataFrame dengan kolom: video_id, username, caption, comment, likes, hashtags, date\n",
        "    \"\"\"\n",
        "    \n",
        "    # IMPLEMENTASI SCRAPING\n",
        "    # Pilih salah satu metode:\n",
        "    # 1. TikTokApi (unofficial)\n",
        "    # 2. RapidAPI TikTok Scraper\n",
        "    # 3. Apify TikTok Scraper\n",
        "    # 4. Playwright/Selenium manual scraping\n",
        "    \n",
        "    all_data = []\n",
        "    \n",
        "    for hashtag in hashtags:\n",
        "        print(f'Scraping #{hashtag}...')\n",
        "        \n",
        "        # TODO: Implementasi scraping sesuai API yang dipilih\n",
        "        # Contoh struktur:\n",
        "        # from TikTokApi import TikTokApi\n",
        "        # api = TikTokApi()\n",
        "        # tag = api.hashtag(name=hashtag)\n",
        "        # videos = tag.videos(count=videos_per_hashtag)\n",
        "        # \n",
        "        # for video in videos:\n",
        "        #     video_data = {\n",
        "        #         'video_id': video.id,\n",
        "        #         'username': video.author.username,\n",
        "        #         'caption': video.desc,\n",
        "        #         'likes': video.stats.diggCount,\n",
        "        #         'hashtags': ' '.join([f'#{tag}' for tag in video.challenges]),\n",
        "        #         'date': datetime.fromtimestamp(video.createTime)\n",
        "        #     }\n",
        "        #     \n",
        "        #     # Get comments\n",
        "        #     comments = video.comments(count=50)\n",
        "        #     for comment in comments:\n",
        "        #         comment_data = video_data.copy()\n",
        "        #         comment_data['comment'] = comment.text\n",
        "        #         all_data.append(comment_data)\n",
        "        \n",
        "        pass\n",
        "    \n",
        "    df = pd.DataFrame(all_data)\n",
        "    return df\n",
        "\n",
        "# Target hashtags\n",
        "hashtags = [\n",
        "    'AIethics', 'blockchain', 'sustainability', 'web3',\n",
        "    'digitalfreedom', 'cryptocurrency', 'NFT', 'metaverse', 'privacy'\n",
        "]\n",
        "\n",
        "# Scrape data\n",
        "print('Memulai scraping...')\n",
        "df_raw = scrape_tiktok_data(hashtags, videos_per_hashtag=100)\n",
        "\n",
        "# Save raw data\n",
        "df_raw.to_csv('output/data/raw_data.csv', index=False)\n",
        "print(f'\\n✓ Data berhasil di-scrape: {len(df_raw)} baris')\n",
        "print(f'✓ Raw data disimpan: output/data/raw_data.csv')\n",
        "print('\\nPreview data:')\n",
        "df_raw.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TAHAP 2: Pra-Pemrosesan Data\n",
        "\n",
        "**Langkah:**\n",
        "1. Case Folding\n",
        "2. Tokenisasi\n",
        "3. Stopword Removal\n",
        "4. Normalisasi (slang → formal)\n",
        "5. Filtering (emoji, URL, mention, hashtag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stopwords bahasa Indonesia\n",
        "stopwords_id = [\n",
        "    'yang', 'dan', 'di', 'ke', 'dari', 'untuk', 'pada', 'dengan', 'adalah',\n",
        "    'ini', 'itu', 'atau', 'juga', 'dalam', 'tidak', 'ada', 'akan', 'oleh',\n",
        "    'saya', 'kamu', 'dia', 'kita', 'mereka', 'kami', 'anda', 'nya', 'mu',\n",
        "    'ku', 'si', 'para', 'sang', 'pak', 'bu', 'bapak', 'ibu', 'mas', 'mbak',\n",
        "    'sudah', 'belum', 'telah', 'masih', 'dapat', 'bisa', 'harus', 'ingin',\n",
        "    'mau', 'bisa', 'jadi', 'jangan', 'kalau', 'kalo', 'bila', 'ketika',\n",
        "    'saat', 'waktu', 'hari', 'tahun', 'bulan', 'minggu', 'jam', 'menit'\n",
        "]\n",
        "\n",
        "# Kamus normalisasi slang TikTok\n",
        "slang_dict = {\n",
        "    'gak': 'tidak', 'ga': 'tidak', 'gk': 'tidak',\n",
        "    'banget': 'sangat', 'bgt': 'sangat', 'bingit': 'sangat',\n",
        "    'keren': 'bagus', 'mantap': 'bagus', 'mantul': 'bagus',\n",
        "    'gimana': 'bagaimana', 'gmn': 'bagaimana', 'gmana': 'bagaimana',\n",
        "    'udah': 'sudah', 'dah': 'sudah', 'udh': 'sudah',\n",
        "    'emang': 'memang', 'emg': 'memang',\n",
        "    'doang': 'saja', 'aja': 'saja', 'aj': 'saja',\n",
        "    'cuma': 'hanya', 'cm': 'hanya',\n",
        "    'hype': 'populer', 'viral': 'populer',\n",
        "    'scam': 'penipuan', 'tipu': 'penipuan',\n",
        "    'bingung': 'membingungkan', 'ribet': 'rumit',\n",
        "    'susah': 'sulit', 'gampang': 'mudah',\n",
        "    'mahal': 'mahal', 'murah': 'murah',\n",
        "    'jelek': 'buruk', 'bagus': 'baik',\n",
        "    'kece': 'bagus', 'oke': 'baik', 'ok': 'baik'\n",
        "}\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocessing teks sesuai metodologi proposal\n",
        "    \"\"\"\n",
        "    if pd.isna(text) or text == '':\n",
        "        return ''\n",
        "    \n",
        "    # 1. Case folding\n",
        "    text = text.lower()\n",
        "    \n",
        "    # 2. Hapus URL\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    \n",
        "    # 3. Hapus mention (@username)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    \n",
        "    # 4. Hapus hashtag (sudah disimpan terpisah)\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "    \n",
        "    # 5. Hapus emoji dan emoticon\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        \"]+\", flags=re.UNICODE)\n",
        "    text = emoji_pattern.sub(r'', text)\n",
        "    \n",
        "    # 6. Hapus tanda baca berlebihan\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "    \n",
        "    # 7. Hapus angka\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    \n",
        "    # 8. Hapus spasi berlebihan\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    \n",
        "    # 9. Tokenisasi\n",
        "    tokens = text.split()\n",
        "    \n",
        "    # 10. Normalisasi slang\n",
        "    tokens = [slang_dict.get(word, word) for word in tokens]\n",
        "    \n",
        "    # 11. Stopword removal\n",
        "    tokens = [word for word in tokens if word not in stopwords_id and len(word) > 2]\n",
        "    \n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply preprocessing\n",
        "print('Memproses caption...')\n",
        "df_raw['processed_caption'] = df_raw['caption'].apply(preprocess_text)\n",
        "\n",
        "print('Memproses comment...')\n",
        "df_raw['processed_comment'] = df_raw['comment'].apply(preprocess_text)\n",
        "\n",
        "# Gabungkan caption dan comment untuk analisis\n",
        "df_raw['processed_text'] = df_raw['processed_caption'] + ' ' + df_raw['processed_comment']\n",
        "df_raw['processed_text'] = df_raw['processed_text'].str.strip()\n",
        "\n",
        "# Remove empty rows\n",
        "df_clean = df_raw[df_raw['processed_text'] != ''].copy()\n",
        "\n",
        "# Save preprocessed data\n",
        "df_clean.to_csv('output/data/preprocessed_data.csv', index=False)\n",
        "\n",
        "print(f'\\n✓ Preprocessing selesai')\n",
        "print(f'✓ Data bersih: {len(df_clean)} baris')\n",
        "print(f'✓ Data disimpan: output/data/preprocessed_data.csv')\n",
        "print('\\nContoh hasil preprocessing:')\n",
        "df_clean[['caption', 'processed_text']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TAHAP 3: Analisis Sentimen (Rule-based)\n",
        "\n",
        "**Metode:** Lexicon-based Sentiment Analysis\n",
        "\n",
        "**Algoritma:**\n",
        "```\n",
        "Skor Sentimen = Σ(kata positif × bobot) - Σ(kata negatif × bobot)\n",
        "```\n",
        "\n",
        "**Klasifikasi:**\n",
        "- Skor > 0 → Positif\n",
        "- Skor = 0 → Netral\n",
        "- Skor < 0 → Negatif"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Kamus sentimen positif dengan bobot\n",
        "positive_words = {\n",
        "    # Bobot 3 (sangat positif)\n",
        "    'hebat': 3, 'luar biasa': 3, 'revolusioner': 3, 'sempurna': 3,\n",
        "    'fantastis': 3, 'menakjubkan': 3, 'brilian': 3,\n",
        "    \n",
        "    # Bobot 2 (positif)\n",
        "    'bagus': 2, 'baik': 2, 'inovatif': 2, 'transparan': 2,\n",
        "    'terdesentralisasi': 2, 'efisien': 2, 'aman': 2, 'solusi': 2,\n",
        "    'masa depan': 2, 'maju': 2, 'modern': 2, 'canggih': 2,\n",
        "    'menarik': 2, 'berguna': 2, 'bermanfaat': 2, 'penting': 2,\n",
        "    'mudah': 2, 'cepat': 2, 'praktis': 2, 'efektif': 2,\n",
        "    'terpercaya': 2, 'kredibel': 2, 'legitimate': 2,\n",
        "    \n",
        "    # Bobot 1 (sedikit positif)\n",
        "    'setuju': 1, 'suka': 1, 'senang': 1, 'tertarik': 1,\n",
        "    'optimis': 1, 'harapan': 1, 'potensi': 1, 'peluang': 1,\n",
        "    'berkembang': 1, 'tumbuh': 1, 'meningkat': 1\n",
        "}\n",
        "\n",
        "# Kamus sentimen negatif dengan bobot\n",
        "negative_words = {\n",
        "    # Bobot 3 (sangat negatif)\n",
        "    'penipuan': 3, 'scam': 3, 'rugi': 3, 'berbahaya': 3,\n",
        "    'manipulasi': 3, 'tidak aman': 3, 'buruk': 3, 'jelek': 3,\n",
        "    'gagal': 3, 'hancur': 3, 'crash': 3, 'bohong': 3,\n",
        "    \n",
        "    # Bobot 2 (negatif)\n",
        "    'membingungkan': 2, 'rumit': 2, 'sulit': 2, 'mahal': 2,\n",
        "    'lambat': 2, 'risiko': 2, 'bahaya': 2, 'ancaman': 2,\n",
        "    'terancam': 2, 'khawatir': 2, 'takut': 2, 'ragu': 2,\n",
        "    'tidak jelas': 2, 'tidak transparan': 2, 'spekulasi': 2,\n",
        "    'volatil': 2, 'tidak stabil': 2,\n",
        "    \n",
        "    # Bobot 1 (sedikit negatif)\n",
        "    'kompleks': 1, 'susah': 1, 'bingung': 1, 'kurang': 1,\n",
        "    'belum': 1, 'masih': 1, 'terbatas': 1, 'lemah': 1\n",
        "}\n",
        "\n",
        "# Save lexicons\n",
        "with open('output/data/positive_lexicon.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(positive_words, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "with open('output/data/negative_lexicon.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(negative_words, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print('✓ Kamus sentimen dibuat dan disimpan')\n",
        "print(f'  - Kata positif: {len(positive_words)}')\n",
        "print(f'  - Kata negatif: {len(negative_words)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_sentiment(text):\n",
        "    \"\"\"\n",
        "    Hitung skor sentimen berdasarkan lexicon\n",
        "    \n",
        "    Returns:\n",
        "    - score: float, skor sentimen\n",
        "    - label: str, label sentimen (positif/netral/negatif)\n",
        "    \"\"\"\n",
        "    if pd.isna(text) or text == '':\n",
        "        return 0, 'netral'\n",
        "    \n",
        "    words = text.split()\n",
        "    \n",
        "    # Hitung skor positif\n",
        "    positive_score = sum(positive_words.get(word, 0) for word in words)\n",
        "    \n",
        "    # Hitung skor negatif\n",
        "    negative_score = sum(negative_words.get(word, 0) for word in words)\n",
        "    \n",
        "    # Total skor\n",
        "    total_score = positive_score - negative_score\n",
        "    \n",
        "    # Klasifikasi\n",
        "    if total_score > 0:\n",
        "        label = 'positif'\n",
        "    elif total_score < 0:\n",
        "        label = 'negatif'\n",
        "    else:\n",
        "        label = 'netral'\n",
        "    \n",
        "    return total_score, label\n",
        "\n",
        "# Apply sentiment analysis\n",
        "print('Melakukan analisis sentimen...')\n",
        "df_clean[['sentiment_score', 'sentiment_label']] = df_clean['processed_text'].apply(\n",
        "    lambda x: pd.Series(calculate_sentiment(x))\n",
        ")\n",
        "\n",
        "# Save results\n",
        "df_clean.to_csv('output/data/sentiment_results.csv', index=False)\n",
        "\n",
        "print('\\n✓ Analisis sentimen selesai')\n",
        "print(f'✓ Hasil disimpan: output/data/sentiment_results.csv')\n",
        "print('\\nDistribusi Sentimen:')\n",
        "print(df_clean['sentiment_label'].value_counts())\n",
        "print('\\nStatistik Skor Sentimen:')\n",
        "print(df_clean['sentiment_score'].describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Bar Chart - Distribusi Sentimen\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "sentiment_counts = df_clean['sentiment_label'].value_counts()\n",
        "colors = {'positif': '#2ecc71', 'netral': '#95a5a6', 'negatif': '#e74c3c'}\n",
        "bars = ax.bar(sentiment_counts.index, sentiment_counts.values, \n",
        "              color=[colors[label] for label in sentiment_counts.index])\n",
        "\n",
        "ax.set_xlabel('Sentimen', fontsize=12)\n",
        "ax.set_ylabel('Jumlah', fontsize=12)\n",
        "ax.set_title('Distribusi Sentimen Publik terhadap Isu Web3', fontsize=14, fontweight='bold')\n",
        "\n",
        "# Add value labels\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{int(height)}\\n({height/len(df_clean)*100:.1f}%)',\n",
        "            ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('output/graphs/sentiment_distribution.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print('✓ Grafik distribusi sentimen disimpan: output/graphs/sentiment_distribution.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TAHAP 4: Analisis Trending Topic\n",
        "\n",
        "**Metode:**\n",
        "1. Frequency Analysis\n",
        "2. TF-IDF Manual\n",
        "3. Identifikasi Tren Temporal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Frequency Analysis\n",
        "all_words = ' '.join(df_clean['processed_text']).split()\n",
        "word_freq = Counter(all_words)\n",
        "top_50_words = word_freq.most_common(50)\n",
        "\n",
        "# Save frequency data\n",
        "freq_df = pd.DataFrame(top_50_words, columns=['word', 'frequency'])\n",
        "freq_df.to_csv('output/data/word_frequency.csv', index=False)\n",
        "\n",
        "print('✓ Analisis frekuensi selesai')\n",
        "print(f'✓ Top 50 kata disimpan: output/data/word_frequency.csv')\n",
        "print('\\nTop 20 Kata Paling Sering Muncul:')\n",
        "print(freq_df.head(20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. TF-IDF Manual Implementation\n",
        "import math\n",
        "\n",
        "def calculate_tf(text):\n",
        "    \"\"\"Calculate Term Frequency\"\"\"\n",
        "    words = text.split()\n",
        "    word_count = Counter(words)\n",
        "    total_words = len(words)\n",
        "    tf = {word: count/total_words for word, count in word_count.items()}\n",
        "    return tf\n",
        "\n",
        "def calculate_idf(documents):\n",
        "    \"\"\"Calculate Inverse Document Frequency\"\"\"\n",
        "    N = len(documents)\n",
        "    idf = {}\n",
        "    \n",
        "    # Get all unique words\n",
        "    all_words = set()\n",
        "    for doc in documents:\n",
        "        all_words.update(doc.split())\n",
        "    \n",
        "    # Calculate IDF for each word\n",
        "    for word in all_words:\n",
        "        doc_count = sum(1 for doc in documents if word in doc.split())\n",
        "        idf[word] = math.log(N / doc_count) if doc_count > 0 else 0\n",
        "    \n",
        "    return idf\n",
        "\n",
        "def calculate_tfidf(documents):\n",
        "    \"\"\"Calculate TF-IDF scores\"\"\"\n",
        "    idf = calculate_idf(documents)\n",
        "    tfidf_scores = []\n",
        "    \n",
        "    for doc in documents:\n",
        "        tf = calculate_tf(doc)\n",
        "        tfidf = {word: tf_score * idf.get(word, 0) for word, tf_score in tf.items()}\n",
        "        tfidf_scores.append(tfidf)\n",
        "    \n",
        "    return tfidf_scores, idf\n",
        "\n",
        "# Calculate TF-IDF\n",
        "print('Menghitung TF-IDF...')\n",
        "documents = df_clean['processed_text'].tolist()\n",
        "tfidf_scores, idf_scores = calculate_tfidf(documents)\n",
        "\n",
        "# Get top TF-IDF words across all documents\n",
        "all_tfidf = {}\n",
        "for tfidf in tfidf_scores:\n",
        "    for word, score in tfidf.items():\n",
        "        all_tfidf[word] = all_tfidf.get(word, 0) + score\n",
        "\n",
        "top_tfidf = sorted(all_tfidf.items(), key=lambda x: x[1], reverse=True)[:50]\n",
        "tfidf_df = pd.DataFrame(top_tfidf, columns=['word', 'tfidf_score'])\n",
        "tfidf_df.to_csv('output/data/tfidf_scores.csv', index=False)\n",
        "\n",
        "print('✓ TF-IDF selesai')\n",
        "print(f'✓ Hasil disimpan: output/data/tfidf_scores.csv')\n",
        "print('\\nTop 20 Kata Berdasarkan TF-IDF:')\n",
        "print(tfidf_df.head(20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Visualisasi Top Words\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Frequency bar chart\n",
        "top_20_freq = freq_df.head(20)\n",
        "ax1.barh(top_20_freq['word'], top_20_freq['frequency'], color='steelblue')\n",
        "ax1.set_xlabel('Frekuensi', fontsize=11)\n",
        "ax1.set_title('Top 20 Kata (Frequency)', fontsize=13, fontweight='bold')\n",
        "ax1.invert_yaxis()\n",
        "\n",
        "# TF-IDF bar chart\n",
        "top_20_tfidf = tfidf_df.head(20)\n",
        "ax2.barh(top_20_tfidf['word'], top_20_tfidf['tfidf_score'], color='coral')\n",
        "ax2.set_xlabel('TF-IDF Score', fontsize=11)\n",
        "ax2.set_title('Top 20 Kata (TF-IDF)', fontsize=13, fontweight='bold')\n",
        "ax2.invert_yaxis()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('output/graphs/top_words_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print('✓ Grafik top words disimpan: output/graphs/top_words_comparison.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TAHAP 5: Visualisasi Wordcloud\n",
        "\n",
        "**Jenis Wordcloud:**\n",
        "1. Wordcloud Keseluruhan\n",
        "2. Wordcloud Per Sentimen (Positif, Negatif, Netral)\n",
        "3. Wordcloud Per Topik"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Wordcloud Keseluruhan\n",
        "all_text = ' '.join(df_clean['processed_text'])\n",
        "\n",
        "wordcloud_all = WordCloud(\n",
        "    width=1600,\n",
        "    height=800,\n",
        "    background_color='white',\n",
        "    colormap='viridis',\n",
        "    max_words=100,\n",
        "    relative_scaling=0.5,\n",
        "    min_font_size=10\n",
        ").generate(all_text)\n",
        "\n",
        "plt.figure(figsize=(20, 10))\n",
        "plt.imshow(wordcloud_all, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Wordcloud Keseluruhan - Isu Web3 di TikTok', fontsize=20, fontweight='bold', pad=20)\n",
        "plt.savefig('output/wordclouds/wordcloud_overall.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print('✓ Wordcloud keseluruhan disimpan: output/wordclouds/wordcloud_overall.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Wordcloud Per Sentimen\n",
        "fig, axes = plt.subplots(1, 3, figsize=(24, 8))\n",
        "\n",
        "sentiments = [\n",
        "    ('positif', 'Greens', axes[0]),\n",
        "    ('netral', 'Greys', axes[1]),\n",
        "    ('negatif', 'Reds', axes[2])\n",
        "]\n",
        "\n",
        "for sentiment, colormap, ax in sentiments:\n",
        "    text_data = ' '.join(df_clean[df_clean['sentiment_label'] == sentiment]['processed_text'])\n",
        "    \n",
        "    if text_data.strip():  # Only create if there's data\n",
        "        wc = WordCloud(\n",
        "            width=1200,\n",
        "            height=800,\n",
        "            background_color='white',\n",
        "            colormap=colormap,\n",
        "            max_words=80,\n",
        "            relative_scaling=0.5,\n",
        "            min_font_size=10\n",
        "        ).generate(text_data)\n",
        "        \n",
        "        ax.imshow(wc, interpolation='bilinear')\n",
        "    \n",
        "    ax.axis('off')\n",
        "    ax.set_title(f'Sentimen {sentiment.upper()}', fontsize=16, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('output/wordclouds/wordcloud_by_sentiment.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print('✓ Wordcloud per sentimen disimpan: output/wordclouds/wordcloud_by_sentiment.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Wordcloud Per Topik\n",
        "# Kategorisasi topik berdasarkan hashtag\n",
        "def categorize_topic(hashtags):\n",
        "    hashtags_lower = hashtags.lower()\n",
        "    if 'ai' in hashtags_lower or 'ethics' in hashtags_lower:\n",
        "        return 'AI Ethics'\n",
        "    elif 'blockchain' in hashtags_lower or 'crypto' in hashtags_lower:\n",
        "        return 'Blockchain & Crypto'\n",
        "    elif 'sustainability' in hashtags_lower or 'green' in hashtags_lower:\n",
        "        return 'Sustainability'\n",
        "    elif 'nft' in hashtags_lower or 'metaverse' in hashtags_lower:\n",
        "        return 'NFT & Metaverse'\n",
        "    elif 'privacy' in hashtags_lower or 'security' in hashtags_lower:\n",
        "        return 'Privacy & Security'\n",
        "    else:\n",
        "        return 'Web3 General'\n",
        "\n",
        "df_clean['topic_category'] = df_clean['hashtags'].apply(categorize_topic)\n",
        "\n",
        "# Save categorized data\n",
        "df_clean.to_csv('output/data/final_data_with_topics.csv', index=False)\n",
        "\n",
        "print('✓ Kategorisasi topik selesai')\n",
        "print('\\nDistribusi Topik:')\n",
        "print(df_clean['topic_category'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create wordcloud for each topic\n",
        "topics = df_clean['topic_category'].unique()\n",
        "n_topics = len(topics)\n",
        "cols = 3\n",
        "rows = (n_topics + cols - 1) // cols\n",
        "\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(20, 6*rows))\n",
        "axes = axes.flatten() if n_topics > 1 else [axes]\n",
        "\n",
        "for idx, topic in enumerate(topics):\n",
        "    text_data = ' '.join(df_clean[df_clean['topic_category'] == topic]['processed_text'])\n",
        "    \n",
        "    if text_data.strip():\n",
        "        wc = WordCloud(\n",
        "            width=1200,\n",
        "            height=600,\n",
        "            background_color='white',\n",
        "            colormap='plasma',\n",
        "            max_words=60,\n",
        "            relative_scaling=0.5,\n",
        "            min_font_size=10\n",
        "        ).generate(text_data)\n",
        "        \n",
        "        axes[idx].imshow(wc, interpolation='bilinear')\n",
        "    \n",
        "    axes[idx].axis('off')\n",
        "    axes[idx].set_title(topic, fontsize=14, fontweight='bold')\n",
        "\n",
        "# Hide unused subplots\n",
        "for idx in range(n_topics, len(axes)):\n",
        "    axes[idx].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('output/wordclouds/wordcloud_by_topic.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print('✓ Wordcloud per topik disimpan: output/wordclouds/wordcloud_by_topic.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TAHAP 6: Analisis Kritis\n",
        "\n",
        "**Fokus Analisis:**\n",
        "1. Kesadaran Web3\n",
        "2. Polarisasi Opini\n",
        "3. Sentimen Per Topik\n",
        "4. Tren Temporal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Analisis Kesadaran Web3\n",
        "web3_keywords = ['web3', 'blockchain', 'crypto', 'cryptocurrency', 'decentralized', \n",
        "                 'terdesentralisasi', 'nft', 'metaverse']\n",
        "\n",
        "# Hitung frekuensi mention\n",
        "keyword_mentions = {}\n",
        "for keyword in web3_keywords:\n",
        "    count = sum(1 for text in df_clean['processed_text'] if keyword in text.lower())\n",
        "    keyword_mentions[keyword] = count\n",
        "\n",
        "# Create DataFrame\n",
        "awareness_df = pd.DataFrame(list(keyword_mentions.items()), \n",
        "                           columns=['keyword', 'mentions'])\n",
        "awareness_df = awareness_df.sort_values('mentions', ascending=False)\n",
        "awareness_df['percentage'] = (awareness_df['mentions'] / len(df_clean) * 100).round(2)\n",
        "\n",
        "# Save\n",
        "awareness_df.to_csv('output/data/web3_awareness.csv', index=False)\n",
        "\n",
        "print('✓ Analisis kesadaran Web3 selesai')\n",
        "print(f'✓ Hasil disimpan: output/data/web3_awareness.csv')\n",
        "print('\\nFrekuensi Mention Istilah Web3:')\n",
        "print(awareness_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Sentimen Per Topik\n",
        "sentiment_by_topic = pd.crosstab(df_clean['topic_category'], \n",
        "                                  df_clean['sentiment_label'], \n",
        "                                  normalize='index') * 100\n",
        "\n",
        "# Save\n",
        "sentiment_by_topic.to_csv('output/data/sentiment_by_topic.csv')\n",
        "\n",
        "# Visualisasi\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "sentiment_by_topic.plot(kind='bar', stacked=True, ax=ax,\n",
        "                       color=['#e74c3c', '#95a5a6', '#2ecc71'])\n",
        "ax.set_xlabel('Topik', fontsize=12)\n",
        "ax.set_ylabel('Persentase (%)', fontsize=12)\n",
        "ax.set_title('Distribusi Sentimen Per Topik', fontsize=14, fontweight='bold')\n",
        "ax.legend(title='Sentimen', labels=['Negatif', 'Netral', 'Positif'])\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('output/graphs/sentiment_by_topic.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print('✓ Analisis sentimen per topik selesai')\n",
        "print(f'✓ Grafik disimpan: output/graphs/sentiment_by_topic.png')\n",
        "print('\\nSentimen Per Topik (%):')\n",
        "print(sentiment_by_topic.round(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Tren Temporal (jika ada data tanggal)\n",
        "if 'date' in df_clean.columns:\n",
        "    df_clean['date'] = pd.to_datetime(df_clean['date'])\n",
        "    df_clean['week'] = df_clean['date'].dt.to_period('W')\n",
        "    \n",
        "    # Sentimen per minggu\n",
        "    weekly_sentiment = df_clean.groupby(['week', 'sentiment_label']).size().unstack(fill_value=0)\n",
        "    \n",
        "    # Plot\n",
        "    fig, ax = plt.subplots(figsize=(14, 6))\n",
        "    weekly_sentiment.plot(ax=ax, marker='o', linewidth=2)\n",
        "    ax.set_xlabel('Minggu', fontsize=12)\n",
        "    ax.set_ylabel('Jumlah', fontsize=12)\n",
        "    ax.set_title('Tren Sentimen dari Waktu ke Waktu', fontsize=14, fontweight='bold')\n",
        "    ax.legend(title='Sentimen')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('output/graphs/sentiment_trend.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    # Save\n",
        "    weekly_sentiment.to_csv('output/data/weekly_sentiment_trend.csv')\n",
        "    \n",
        "    print('✓ Analisis tren temporal selesai')\n",
        "    print(f'✓ Grafik disimpan: output/graphs/sentiment_trend.png')\n",
        "else:\n",
        "    print('⚠ Data tanggal tidak tersedia, skip analisis temporal')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Polarisasi Opini\n",
        "# Hitung tingkat polarisasi per topik\n",
        "polarization = df_clean.groupby('topic_category').agg({\n",
        "    'sentiment_score': ['mean', 'std', 'min', 'max']\n",
        "}).round(2)\n",
        "\n",
        "polarization.columns = ['mean_score', 'std_score', 'min_score', 'max_score']\n",
        "polarization['polarization_index'] = polarization['std_score']  # Std sebagai indikator polarisasi\n",
        "polarization = polarization.sort_values('polarization_index', ascending=False)\n",
        "\n",
        "# Save\n",
        "polarization.to_csv('output/data/polarization_analysis.csv')\n",
        "\n",
        "print('✓ Analisis polarisasi selesai')\n",
        "print(f'✓ Hasil disimpan: output/data/polarization_analysis.csv')\n",
        "print('\\nIndeks Polarisasi Per Topik:')\n",
        "print(polarization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TAHAP 7: Ringkasan dan Kesimpulan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate Summary Report\n",
        "summary_report = f\"\"\"\n",
        "=================================================================\n",
        "LAPORAN ANALISIS SENTIMEN TIKTOK - ISU WEB3\n",
        "=================================================================\n",
        "\n",
        "1. RINGKASAN DATA\n",
        "   - Total data terkumpul: {len(df_raw)} baris\n",
        "   - Data setelah preprocessing: {len(df_clean)} baris\n",
        "   - Periode data: {df_clean['date'].min()} s/d {df_clean['date'].max() if 'date' in df_clean.columns else 'N/A'}\n",
        "\n",
        "2. DISTRIBUSI SENTIMEN\n",
        "{df_clean['sentiment_label'].value_counts().to_string()}\n",
        "\n",
        "Persentase:\n",
        "{(df_clean['sentiment_label'].value_counts() / len(df_clean) * 100).round(2).to_string()}\n",
        "\n",
        "3. STATISTIK SKOR SENTIMEN\n",
        "{df_clean['sentiment_score'].describe().to_string()}\n",
        "\n",
        "4. TOP 10 KATA PALING SERING MUNCUL\n",
        "{freq_df.head(10).to_string(index=False)}\n",
        "\n",
        "5. DISTRIBUSI TOPIK\n",
        "{df_clean['topic_category'].value_counts().to_string()}\n",
        "\n",
        "6. KESADARAN WEB3\n",
        "{awareness_df.to_string(index=False)}\n",
        "\n",
        "7. SENTIMEN PER TOPIK (%)\n",
        "{sentiment_by_topic.round(2).to_string()}\n",
        "\n",
        "8. POLARISASI OPINI\n",
        "{polarization.to_string()}\n",
        "\n",
        "=================================================================\n",
        "KESIMPULAN\n",
        "=================================================================\n",
        "\n",
        "Berdasarkan analisis data TikTok mengenai isu Web3:\n",
        "\n",
        "1. Sentimen Dominan:\n",
        "   - Sentimen {df_clean['sentiment_label'].value_counts().index[0]} mendominasi \n",
        "     dengan {df_clean['sentiment_label'].value_counts().values[0]} komentar \n",
        "     ({df_clean['sentiment_label'].value_counts().values[0]/len(df_clean)*100:.1f}%)\n",
        "\n",
        "2. Topik Paling Dibahas:\n",
        "   - {df_clean['topic_category'].value_counts().index[0]} \n",
        "     ({df_clean['topic_category'].value_counts().values[0]} mentions)\n",
        "\n",
        "3. Kesadaran Web3:\n",
        "   - Istilah paling sering disebut: {awareness_df.iloc[0]['keyword']} \n",
        "     ({awareness_df.iloc[0]['mentions']} kali, {awareness_df.iloc[0]['percentage']:.1f}%)\n",
        "\n",
        "4. Polarisasi:\n",
        "   - Topik paling kontroversial: {polarization.index[0]} \n",
        "     (std: {polarization.iloc[0]['std_score']})\n",
        "\n",
        "=================================================================\n",
        "FILE OUTPUT YANG DIHASILKAN\n",
        "=================================================================\n",
        "\n",
        "DATA:\n",
        "  ✓ output/data/raw_data.csv\n",
        "  ✓ output/data/preprocessed_data.csv\n",
        "  ✓ output/data/sentiment_results.csv\n",
        "  ✓ output/data/final_data_with_topics.csv\n",
        "  ✓ output/data/word_frequency.csv\n",
        "  ✓ output/data/tfidf_scores.csv\n",
        "  ✓ output/data/web3_awareness.csv\n",
        "  ✓ output/data/sentiment_by_topic.csv\n",
        "  ✓ output/data/polarization_analysis.csv\n",
        "  ✓ output/data/positive_lexicon.json\n",
        "  ✓ output/data/negative_lexicon.json\n",
        "\n",
        "GRAFIK:\n",
        "  ✓ output/graphs/sentiment_distribution.png\n",
        "  ✓ output/graphs/top_words_comparison.png\n",
        "  ✓ output/graphs/sentiment_by_topic.png\n",
        "  ✓ output/graphs/sentiment_trend.png (jika ada data temporal)\n",
        "\n",
        "WORDCLOUD:\n",
        "  ✓ output/wordclouds/wordcloud_overall.png\n",
        "  ✓ output/wordclouds/wordcloud_by_sentiment.png\n",
        "  ✓ output/wordclouds/wordcloud_by_topic.png\n",
        "\n",
        "=================================================================\n",
        "\"\"\"\n",
        "\n",
        "# Save summary report\n",
        "with open('output/SUMMARY_REPORT.txt', 'w', encoding='utf-8') as f:\n",
        "    f.write(summary_report)\n",
        "\n",
        "print(summary_report)\n",
        "print('\\n✓ Laporan ringkasan disimpan: output/SUMMARY_REPORT.txt')\n",
        "print('\\n' + '='*65)\n",
        "print('ANALISIS SELESAI!')\n",
        "print('='*65)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Catatan Implementasi\n",
        "\n",
        "### Untuk Scraping Data Real:\n",
        "\n",
        "1. **TikTokApi (Unofficial)**\n",
        "   ```bash\n",
        "   pip install TikTokApi\n",
        "   playwright install\n",
        "   ```\n",
        "\n",
        "2. **RapidAPI TikTok Scraper**\n",
        "   - Daftar di https://rapidapi.com\n",
        "   - Subscribe ke TikTok Scraper API\n",
        "   - Gunakan API key untuk request\n",
        "\n",
        "3. **Apify TikTok Scraper**\n",
        "   - Daftar di https://apify.com\n",
        "   - Gunakan TikTok Scraper actor\n",
        "   - Export hasil ke CSV/JSON\n",
        "\n",
        "### Modifikasi yang Diperlukan:\n",
        "\n",
        "- Ganti fungsi `scrape_tiktok_data()` dengan implementasi API yang dipilih\n",
        "- Sesuaikan struktur data dengan response API\n",
        "- Tambahkan error handling dan rate limiting\n",
        "- Implementasikan data privacy (enkripsi username)\n",
        "\n",
        "### Referensi:\n",
        "\n",
        "- Proposal: `tiktok_sentiment_proposal.md`\n",
        "- TikTokApi Docs: https://github.com/davidteather/TikTok-Api\n",
        "- WordCloud Docs: https://amueller.github.io/word_cloud/\n",
        "\n",
        "---\n",
        "\n",
        "**Dibuat sesuai metodologi penelitian dalam proposal**\n",
        "\n",
        "**Semua output disimpan di folder `output/`**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
